{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.2.0 (SDL 2.0.22, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from game import SnakeGame, ACTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9               \n",
    "BATCH_SIZE = 1000          \n",
    "BUFFER_SIZE = 100_000      \n",
    "MIN_REPLAY_SIZE = 1_000    \n",
    "EPSILON_START = 1.0 \n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 10_000      \n",
    "TARGET_UPDATE_FREQ = 1_000  \n",
    "LEARNING_RATE = 0.001\n",
    "TARGET_SAVE_FREQ = TARGET_UPDATE_FREQ*25\n",
    "MODELS_DIR = '../saved_models'\n",
    "LOG_DIR = './logs/snake_3_1'\n",
    "DOUBLE = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(env.get_observation_space_size(), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, env.get_action_space_size()),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    def act(self,obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self.forward(obs_t.unsqueeze(0))\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeGame()\n",
    "\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "reward_buffer = deque([],maxlen=100)\n",
    "score_buffer = deque([],maxlen=100)\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniziatlize Replay Buffer\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.sample_from_action_space()\n",
    "\n",
    "    new_obs, rew, done, _ = env.step(action)\n",
    "    \n",
    "    \n",
    "    transition = (obs,action,rew,done,new_obs)\n",
    "    \n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving target net\n",
      "\n",
      "Step 0\n",
      "Avg Rew nan\n",
      "Avg Score nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teodorosullazzo/opt/anaconda3/envs/pytorch_env_2/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/teodorosullazzo/opt/anaconda3/envs/pytorch_env_2/lib/python3.9/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000\n",
      "Avg Rew -10.0\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 2000\n",
      "Avg Rew -10.0\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 3000\n",
      "Avg Rew -10.0\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 4000\n",
      "Avg Rew -9.545454545454545\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 5000\n",
      "Avg Rew -9.310344827586206\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 6000\n",
      "Avg Rew -9.117647058823529\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 7000\n",
      "Avg Rew -8.461538461538462\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 8000\n",
      "Avg Rew -7.708333333333333\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 9000\n",
      "Avg Rew -6.111111111111111\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 10000\n",
      "Avg Rew -5.535714285714286\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 11000\n",
      "Avg Rew -5.535714285714286\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 12000\n",
      "Avg Rew -5.535714285714286\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 13000\n",
      "Avg Rew -5.535714285714286\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 14000\n",
      "Avg Rew -4.385964912280702\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 15000\n",
      "Avg Rew -3.9655172413793105\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 16000\n",
      "Avg Rew -1.3333333333333333\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 17000\n",
      "Avg Rew -1.3333333333333333\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 18000\n",
      "Avg Rew -0.16393442622950818\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 19000\n",
      "Avg Rew 1.1290322580645162\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 20000\n",
      "Avg Rew 2.8125\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 21000\n",
      "Avg Rew 5.151515151515151\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 22000\n",
      "Avg Rew 5.151515151515151\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 23000\n",
      "Avg Rew 7.014925373134329\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 24000\n",
      "Avg Rew 8.823529411764707\n",
      "Avg Score 1.0\n",
      "Saving target net\n",
      "\n",
      "Step 25000\n",
      "Avg Rew 8.823529411764707\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 26000\n",
      "Avg Rew 8.823529411764707\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 27000\n",
      "Avg Rew 8.823529411764707\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 28000\n",
      "Avg Rew 8.823529411764707\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 29000\n",
      "Avg Rew 16.714285714285715\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 30000\n",
      "Avg Rew 17.746478873239436\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 31000\n",
      "Avg Rew 19.444444444444443\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 32000\n",
      "Avg Rew 20.54794520547945\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 33000\n",
      "Avg Rew 24.133333333333333\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 34000\n",
      "Avg Rew 24.133333333333333\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 35000\n",
      "Avg Rew 28.83116883116883\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 36000\n",
      "Avg Rew 29.23076923076923\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 37000\n",
      "Avg Rew 29.23076923076923\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 38000\n",
      "Avg Rew 29.23076923076923\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 39000\n",
      "Avg Rew 29.23076923076923\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 40000\n",
      "Avg Rew 29.23076923076923\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 41000\n",
      "Avg Rew 43.29113924050633\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 42000\n",
      "Avg Rew 43.29113924050633\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 43000\n",
      "Avg Rew 48.125\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 44000\n",
      "Avg Rew 48.125\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 45000\n",
      "Avg Rew 48.125\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 46000\n",
      "Avg Rew 57.03703703703704\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 47000\n",
      "Avg Rew 57.03703703703704\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 48000\n",
      "Avg Rew 64.26829268292683\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 49000\n",
      "Avg Rew 64.26829268292683\n",
      "Avg Score 1.0\n",
      "Saving target net\n",
      "\n",
      "Step 50000\n",
      "Avg Rew 68.79518072289157\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 51000\n",
      "Avg Rew 69.05882352941177\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 52000\n",
      "Avg Rew 69.05882352941177\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 53000\n",
      "Avg Rew 73.2183908045977\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 54000\n",
      "Avg Rew 73.2183908045977\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 55000\n",
      "Avg Rew 73.2183908045977\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 56000\n",
      "Avg Rew 84.20454545454545\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 57000\n",
      "Avg Rew 84.20454545454545\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 58000\n",
      "Avg Rew 84.20454545454545\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 59000\n",
      "Avg Rew 84.20454545454545\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 60000\n",
      "Avg Rew 84.20454545454545\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 61000\n",
      "Avg Rew 99.7752808988764\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 62000\n",
      "Avg Rew 99.7752808988764\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 63000\n",
      "Avg Rew 99.7752808988764\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 64000\n",
      "Avg Rew 99.7752808988764\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 65000\n",
      "Avg Rew 99.7752808988764\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 66000\n",
      "Avg Rew 99.7752808988764\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 67000\n",
      "Avg Rew 118.66666666666667\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 68000\n",
      "Avg Rew 118.66666666666667\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 69000\n",
      "Avg Rew 126.5934065934066\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 70000\n",
      "Avg Rew 126.5934065934066\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 71000\n",
      "Avg Rew 126.5934065934066\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 72000\n",
      "Avg Rew 126.5934065934066\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 73000\n",
      "Avg Rew 136.41304347826087\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 74000\n",
      "Avg Rew 136.41304347826087\n",
      "Avg Score 1.0\n",
      "Saving target net\n",
      "\n",
      "Step 75000\n",
      "Avg Rew 136.41304347826087\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 76000\n",
      "Avg Rew 136.41304347826087\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 77000\n",
      "Avg Rew 147.41935483870967\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 78000\n",
      "Avg Rew 147.41935483870967\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 79000\n",
      "Avg Rew 147.41935483870967\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 80000\n",
      "Avg Rew 155.63829787234042\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 81000\n",
      "Avg Rew 155.63829787234042\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 82000\n",
      "Avg Rew 160.8421052631579\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 83000\n",
      "Avg Rew 160.8421052631579\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 84000\n",
      "Avg Rew 160.8421052631579\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 85000\n",
      "Avg Rew 173.02083333333334\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 86000\n",
      "Avg Rew 172.98969072164948\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 87000\n",
      "Avg Rew 172.98969072164948\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 88000\n",
      "Avg Rew 172.98969072164948\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 89000\n",
      "Avg Rew 180.81632653061226\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 90000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 91000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 92000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 93000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 94000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 95000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 96000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 97000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 98000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 99000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "Saving target net\n",
      "\n",
      "Step 100000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 101000\n",
      "Avg Rew 185.75757575757575\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 102000\n",
      "Avg Rew 226.5\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 103000\n",
      "Avg Rew 228.9\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 104000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 105000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 106000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 107000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 108000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 109000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 110000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 111000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 112000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 113000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 114000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 115000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 116000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 117000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 118000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 119000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 120000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 121000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 122000\n",
      "Avg Rew 231.4\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 123000\n",
      "Avg Rew 300.9\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 124000\n",
      "Avg Rew 301.5\n",
      "Avg Score 1.0\n",
      "Saving target net\n",
      "\n",
      "Step 125000\n",
      "Avg Rew 301.5\n",
      "Avg Score 1.0\n",
      "\n",
      "Step 126000\n",
      "Avg Rew 301.5\n",
      "Avg Score 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m     episode_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     29\u001b[0m \u001b[39m# start gradient step \u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m transitions \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39;49msample(replay_buffer,BATCH_SIZE)\n\u001b[1;32m     32\u001b[0m obses \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([t[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m transitions])\n\u001b[1;32m     33\u001b[0m actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray([t[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m transitions])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch_env_2/lib/python3.9/random.py:470\u001b[0m, in \u001b[0;36mRandom.sample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    468\u001b[0m             j \u001b[39m=\u001b[39m randbelow(n)\n\u001b[1;32m    469\u001b[0m         selected_add(j)\n\u001b[0;32m--> 470\u001b[0m         result[i] \u001b[39m=\u001b[39m population[j]\n\u001b[1;32m    471\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in itertools.count():\n",
    "    epsilon = np.interp(step,[0,EPSILON_DECAY],[EPSILON_START,EPSILON_END])\n",
    "    rnd_sample = random.random()\n",
    "\n",
    "    if rnd_sample < epsilon:\n",
    "        action = env.sample_from_action_space()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "\n",
    "    new_obs, rew, done, infos = env.step(action)\n",
    "\n",
    "    transition = (obs,action,rew,done,new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    episode_reward += rew\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        score_buffer.append(infos['score'])\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "\n",
    "    # start gradient step \n",
    "    transitions = random.sample(replay_buffer,BATCH_SIZE)\n",
    "    \n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rewards = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "\n",
    "    obses_t = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    \n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "\n",
    "    # compute targets\n",
    "    with torch.no_grad():\n",
    "        if DOUBLE:\n",
    "            target_online_q_values = online_net(new_obses_t)\n",
    "            target_online_best_q_indeces = target_online_q_values.argmax(dim=1,keepdim=True)\n",
    "\n",
    "            targets_target_q_values = target_net(new_obses_t)\n",
    "            targets_selected_q_values = torch.gather(input=targets_target_q_values,dim=1,index=target_online_best_q_indeces)\n",
    "            targets = rewards_t + GAMMA * (1-dones_t) * targets_selected_q_values\n",
    "        else:\n",
    "            target_q_values = target_net(new_obses_t)\n",
    "            max_target_q_values = target_q_values.max(dim=1,keepdim=True)[0]\n",
    "            targets = rewards_t + GAMMA * (1-dones_t) * max_target_q_values\n",
    "\n",
    "    # loss\n",
    "\n",
    "    q_values = online_net(obses_t)\n",
    "\n",
    "    action_q_values = torch.gather(input = q_values, dim=1, index = actions_t)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values,targets)\n",
    "\n",
    "    # gradient step\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update target network if needed\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    # checkpointing\n",
    "    if step % TARGET_SAVE_FREQ == 0:\n",
    "        print(\"Saving target net\")\n",
    "        torch.save(target_net.state_dict(), MODELS_DIR+\"/snake_target_net_.pth\")\n",
    "    \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        rew_mean = np.mean(reward_buffer)\n",
    "        score_mean = np.mean(score_buffer)\n",
    "        print()\n",
    "        print('Step', step)\n",
    "        print('Avg Rew',rew_mean)\n",
    "        print('Avg Score',score_mean)\n",
    "        summary_writer.add_scalar('avg_rew', rew_mean, global_step=step)\n",
    "        summary_writer.add_scalar('avg_score', score_mean, global_step=step)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint_step(target_net,optimizer,step,replay_buffer,reward_buffer,score_buffer):\n",
    "    checkpoint = {\n",
    "        'target_net': target_net.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        'step': step,\n",
    "        'replay_buffer': replay_buffer,\n",
    "        'reward_buffer': reward_buffer,\n",
    "        'score_buffer': score_buffer\n",
    "    }\n",
    "    torch.save(checkpoint, MODELS_DIR+\"/snake_checkpoint_step_{}.pth\".format(step))\n",
    "\n",
    "save_checkpoint_step(target_net,optimizer,step,replay_buffer,reward_buffer,score_buffer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
