{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch import nn\n",
    "import torch\n",
    "import gym\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "from pytorch_wrappers import make_atari_deepmind, BatchedPytorchFrameStack, PytorchLazyFrames\n",
    "from baselines_wrappers.dummy_vec_env import DummyVecEnv\n",
    "from baselines_wrappers.monitor import Monitor\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99                # discount rate\n",
    "BATCH_SIZE = 32             # quanti elementi dal replay buffer\n",
    "BUFFER_SIZE = int(1e6)        # dimensione del replay buffer, superato questo valore, i vecchi elementi vengono sovrascritti\n",
    "MIN_REPLAY_SIZE = 50_000     # quanti elementi sono necessari prima di iniziare la discesa del gradiente\n",
    "EPSILON_START = 1.0 \n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = int(1e6)      # quanti episodi per arrivare da EPSILON_START a EPSILON_END\n",
    "TARGET_UPDATE_FREQ = 10_000  # ogni quanti episodi aggiorno Q con Q^\n",
    "LEARNING_RATE = 2.5e-4\n",
    "SAVE_INTERVAL = 10_000\n",
    "MODELS_DIR = './saved_models'\n",
    "SAVE_PATH = os.path.join(MODELS_DIR, 'atari_model.pth')\n",
    "NUM_ENVS = 4\n",
    "LOG_DIR = './logs/atari_vanilla'\n",
    "LOG_INTERVAL = 1_000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nature_cnn(observation_space, depths = (32,64,64),final_layer=512):\n",
    "    n_input_channels = observation_space.shape[0]\n",
    "\n",
    "    cnn = nn.Sequential(\n",
    "        nn.Conv2d(n_input_channels, depths[0], kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(depths[0], depths[1], kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(depths[1], depths[2], kernel_size=3, stride=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten(),\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        n_flatten = cnn(torch.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "    out = nn.Sequential(\n",
    "        cnn,\n",
    "        nn.Linear(n_flatten, final_layer),\n",
    "        nn.ReLU()\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,env,device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.num_actions = env.action_space.n\n",
    "        \n",
    "        conv_net = nature_cnn(env.observation_space)\n",
    "        self.net = nn.Sequential(\n",
    "            conv_net,\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    def act(self,obses,epsilon):\n",
    "        obses_t = torch.as_tensor(obses, dtype=torch.float32, device=device)\n",
    "        q_values = self(obses_t)\n",
    "        # perchè è un batch di un solo elemento\n",
    "        max_q_indeces = torch.argmax(q_values, dim=1)\n",
    "        actions = max_q_indeces.detach().tolist()\n",
    "        for i in range(len(actions)):\n",
    "            rnd_sample = random.random()\n",
    "            if rnd_sample <= epsilon:\n",
    "                actions[i] = random.randint(0,self.num_actions-1)\n",
    "        return actions\n",
    "    \n",
    "    def save(self, path):\n",
    "        \n",
    "        torch.save(self.state_dict(), path)\n",
    "\n",
    "    def load(self, path):\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(path)\n",
    "        self.load_state_dict(torch.load(path))\n",
    "    \n",
    "    def compute_loss(self, transitions, target_net):\n",
    "        obses = [t[0] for t in transitions] #Pythonlazyframes\n",
    "        actions = np.asarray([t[1] for t in transitions])\n",
    "        rewards = np.asarray([t[2] for t in transitions])\n",
    "        dones = np.asarray([t[3] for t in transitions])\n",
    "        new_obses = [t[4] for t in transitions]\n",
    "\n",
    "        if isinstance(obses[0],PytorchLazyFrames):\n",
    "            obses = np.stack([o.get_frames() for o in obses])\n",
    "            new_obses = np.stack([og.get_frames() for og in new_obses])\n",
    "        else:\n",
    "            obses = np.asarray(obses)\n",
    "            new_obses = np.asarray(new_obses)\n",
    "\n",
    "        obses_t = torch.as_tensor(obses, dtype=torch.float32,device=device)\n",
    "        # qui è unsqueeze(-1) perchè è un batch effettivo e quindi dobbiamo\n",
    "        # fare il resize alla dimensione del batch\n",
    "        actions_t = torch.as_tensor(actions, dtype=torch.int64,device=device).unsqueeze(-1)\n",
    "        rewards_t = torch.as_tensor(rewards, dtype=torch.float32,device=device).unsqueeze(-1)\n",
    "        dones_t = torch.as_tensor(dones, dtype=torch.float32,device=device).unsqueeze(-1)\n",
    "        new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32,device=device)\n",
    "\n",
    "        # compute targets\n",
    "\n",
    "        target_q_values = target_net(new_obses_t)\n",
    "        # ogni predizione restituisce il q value per ogni azione, prendiamo solo\n",
    "        # quello per l'azione scelta\n",
    "        max_target_q_values = target_q_values.max(dim=1,keepdim=True)[0]\n",
    "\n",
    "        targets = rewards_t + GAMMA * (1-dones_t) * max_target_q_values\n",
    "\n",
    "        # compute loss\n",
    "\n",
    "        q_values = self(obses_t)\n",
    "        # in questo caso dobbiamo prendere il q value per l'azione scelta\n",
    "        # che potrebbe essere diversa da quella massima per il passo random\n",
    "        action_q_values = torch.gather(input = q_values, dim=1, index = actions_t)\n",
    "\n",
    "        loss = nn.functional.smooth_l1_loss(action_q_values,targets)\n",
    "\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_env = lambda : Monitor(make_atari_deepmind('Breakout-v0'),allow_early_resets=True)\n",
    "vec_env = DummyVecEnv([make_env for _ in range(NUM_ENVS)])\n",
    "# use subprocvecenv after debugging\n",
    "# env = SubprocVecEnv([make_env for _ in NUM_ENVS])\n",
    "\n",
    "# the observation returned by BatchedPytorchFrameStack is LazyFrames\n",
    "env = BatchedPytorchFrameStack(vec_env,k=4)\n",
    "\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "epinfos_buffer = deque([],maxlen=100)\n",
    "\n",
    "episode_count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_net = Network(env,device).to(device)\n",
    "target_net = Network(env,device).to(device)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniziatlize Replay Buffer\n",
    "obses = env.reset()\n",
    "\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    # we want to sample multiple action\n",
    "\n",
    "    actions = [env.action_space.sample() for _ in range(NUM_ENVS)]\n",
    "\n",
    "    new_obses, rews, dones, _ = env.step(actions)\n",
    "    for obs, action, rew, done, new_obs in zip(obses, actions, rews, dones, new_obses):\n",
    "        transition = (obs,action,rew,done,new_obs)\n",
    "        replay_buffer.append(transition)\n",
    "        \n",
    "    obses = new_obses\n",
    "    \n",
    "    # the dummy vec env reset the environment\n",
    "    # automatically, no need to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "obses = env.reset()\n",
    "\n",
    "for step in itertools.count():\n",
    "    epsilon = np.interp(step * NUM_ENVS,[0,EPSILON_DECAY],[EPSILON_START,EPSILON_END])\n",
    "    rnd_sample = random.random()\n",
    "\n",
    "    if isinstance(obses[0],PytorchLazyFrames):\n",
    "        act_obses = np.stack([o.get_frames() for o in obses])\n",
    "        actions = online_net.act(act_obses,epsilon)\n",
    "    else:\n",
    "        actions = online_net.act(obses,epsilon)\n",
    "\n",
    "    # we make the random choice in the net\n",
    "    new_obses, rews, dones, infos = env.step(actions)\n",
    "\n",
    "    for obs, action, rew, done, new_obs, info in zip(obses, actions, rews, dones, new_obses, infos):\n",
    "        transition = (obs,action,rew,done,new_obs)\n",
    "        replay_buffer.append(transition)\n",
    "        if done:\n",
    "            epinfos_buffer.append(info['episode'])\n",
    "            episode_count +=1\n",
    "    \n",
    "    obses = new_obses\n",
    "\n",
    "    # start gradient step \n",
    "    transitions = random.sample(replay_buffer,BATCH_SIZE)\n",
    "    # si usa il numpy perch torch è più veloce sui numpy\n",
    "    \n",
    "    loss = online_net.compute_loss(transitions,target_net)\n",
    "    # gradient descent\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update target network\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    # Logging\n",
    "    if step % LOG_INTERVAL == 0:\n",
    "        # epinfos_buffer is a deque of dicts, r and l are, respectively, the\n",
    "        # reward and length of the episode\n",
    "        rew_mean = np.mean([epinfo['r'] for epinfo in epinfos_buffer]) or 0\n",
    "        len_mean = np.mean([epinfo['l'] for epinfo in epinfos_buffer]) or 0\n",
    "        print()\n",
    "        print('Step', step)\n",
    "        print('Avg rew', rew_mean)\n",
    "        print('Avg ep len', len_mean)\n",
    "        print('Episodes', episode_count)\n",
    "\n",
    "        # tensorboard\n",
    "        summary_writer.add_scalar('avg_rew', rew_mean, global_step=step)\n",
    "        summary_writer.add_scalar('avg_ep_len', len_mean, global_step=step)\n",
    "        summary_writer.add_scalar('episodes', episode_count, global_step=step)\n",
    "        \n",
    "\n",
    "    # saving\n",
    "    if step % SAVE_INTERVAL == 0 and step!=0:\n",
    "        print('Saving model')\n",
    "        online_net.save(SAVE_PATH)\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
