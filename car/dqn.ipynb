{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.0 (SDL 2.0.16, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from car import CarEnv,WIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99               \n",
    "BATCH_SIZE = 64          \n",
    "BUFFER_SIZE = 200_000      \n",
    "MIN_REPLAY_SIZE = 1_000    \n",
    "EPSILON_START = 1.0 \n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 10_000      \n",
    "TARGET_UPDATE_FREQ = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "TARGET_SAVE_FREQ = TARGET_UPDATE_FREQ*25\n",
    "MODELS_DIR = '../saved_models'\n",
    "LOG_DIR = '../logs/car_6_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(env.get_observation_space_size(), 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, env.get_action_space_size()),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    def act(self,obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self.forward(obs_t.unsqueeze(0))\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CarEnv()\n",
    "\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "reward_buffer = deque([],maxlen=100)\n",
    "score_buffer = deque([],maxlen=100)\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniziatlize Replay Buffer\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.sample_from_action_space()\n",
    "\n",
    "    new_obs, rew, done = env.step(action)    \n",
    "    \n",
    "    transition = (obs,action,rew,done,new_obs)\n",
    "    \n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving target net\n",
      "\n",
      "Step 0\n",
      "Avg Rew nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teodorosullazzo/opt/anaconda3/envs/pytorch_env_2/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/teodorosullazzo/opt/anaconda3/envs/pytorch_env_2/lib/python3.9/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000\n",
      "Avg Rew 0.5263157894736842\n",
      "\n",
      "Step 2000\n",
      "Avg Rew 0.4358974358974359\n",
      "\n",
      "Step 3000\n",
      "Avg Rew 0.6\n",
      "\n",
      "Step 4000\n",
      "Avg Rew 0.7464788732394366\n",
      "\n",
      "Step 5000\n",
      "Avg Rew 0.872093023255814\n",
      "\n",
      "Step 6000\n",
      "Avg Rew 0.9897959183673469\n",
      "\n",
      "Step 7000\n",
      "Avg Rew 1.13\n",
      "\n",
      "Step 8000\n",
      "Avg Rew 1.32\n",
      "\n",
      "Step 9000\n",
      "Avg Rew 1.59\n",
      "\n",
      "Step 10000\n",
      "Avg Rew 1.8\n",
      "\n",
      "Step 11000\n",
      "Avg Rew 1.93\n",
      "\n",
      "Step 12000\n",
      "Avg Rew 2.17\n",
      "\n",
      "Step 13000\n",
      "Avg Rew 2.37\n",
      "\n",
      "Step 14000\n",
      "Avg Rew 2.34\n",
      "\n",
      "Step 15000\n",
      "Avg Rew 2.82\n",
      "\n",
      "Step 16000\n",
      "Avg Rew 2.99\n",
      "\n",
      "Step 17000\n",
      "Avg Rew 3.41\n",
      "\n",
      "Step 18000\n",
      "Avg Rew 3.68\n",
      "\n",
      "Step 19000\n",
      "Avg Rew 3.99\n",
      "\n",
      "Step 20000\n",
      "Avg Rew 4.14\n",
      "\n",
      "Step 21000\n",
      "Avg Rew 4.14\n",
      "\n",
      "Step 22000\n",
      "Avg Rew 4.14\n",
      "\n",
      "Step 23000\n",
      "Avg Rew 4.14\n",
      "\n",
      "Step 24000\n",
      "Avg Rew 5.39\n",
      "Saving target net\n",
      "\n",
      "Step 25000\n",
      "Avg Rew 5.63\n",
      "\n",
      "Step 26000\n",
      "Avg Rew 5.74\n",
      "\n",
      "Step 27000\n",
      "Avg Rew 6.19\n",
      "\n",
      "Step 28000\n",
      "Avg Rew 6.39\n",
      "\n",
      "Step 29000\n",
      "Avg Rew 6.69\n",
      "\n",
      "Step 30000\n",
      "Avg Rew 6.9\n",
      "\n",
      "Step 31000\n",
      "Avg Rew 7.17\n",
      "\n",
      "Step 32000\n",
      "Avg Rew 7.23\n",
      "\n",
      "Step 33000\n",
      "Avg Rew 7.51\n",
      "\n",
      "Step 34000\n",
      "Avg Rew 7.67\n",
      "\n",
      "Step 35000\n",
      "Avg Rew 7.82\n",
      "\n",
      "Step 36000\n",
      "Avg Rew 8.19\n",
      "\n",
      "Step 37000\n",
      "Avg Rew 8.32\n",
      "\n",
      "Step 38000\n",
      "Avg Rew 8.42\n",
      "\n",
      "Step 39000\n",
      "Avg Rew 8.7\n",
      "\n",
      "Step 40000\n",
      "Avg Rew 8.99\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     action \u001b[39m=\u001b[39m online_net\u001b[39m.\u001b[39mact(obs)\n\u001b[0;32m---> 14\u001b[0m new_obs, rew, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     17\u001b[0m transition \u001b[39m=\u001b[39m (obs,action,rew,done,new_obs)\n\u001b[1;32m     18\u001b[0m replay_buffer\u001b[39m.\u001b[39mappend(transition)\n",
      "File \u001b[0;32m~/Documents/git_repos/self_driving/car/car.py:366\u001b[0m, in \u001b[0;36mCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mGame is over\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    364\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m--> 366\u001b[0m draw(WIN, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimages, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplayer_car)\n\u001b[1;32m    368\u001b[0m \u001b[39mif\u001b[39;00m action \u001b[39m==\u001b[39m ACTIONS[\u001b[39m\"\u001b[39m\u001b[39mLEFT\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_car\u001b[39m.\u001b[39mrotate(left\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/git_repos/self_driving/car/car.py:105\u001b[0m, in \u001b[0;36mdraw\u001b[0;34m(win, images, player_car)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw\u001b[39m(win, images, player_car):\n\u001b[1;32m    104\u001b[0m     \u001b[39mfor\u001b[39;00m img, pos \u001b[39min\u001b[39;00m images:\n\u001b[0;32m--> 105\u001b[0m         win\u001b[39m.\u001b[39;49mblit(img, pos)\n\u001b[1;32m    107\u001b[0m     player_car\u001b[39m.\u001b[39mdraw(win)\n\u001b[1;32m    108\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mupdate()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in itertools.count():\n",
    "    epsilon = np.interp(step,[0,EPSILON_DECAY],[EPSILON_START,EPSILON_END])\n",
    "    rnd_sample = random.random()\n",
    "\n",
    "    if rnd_sample < epsilon:\n",
    "        action = env.sample_from_action_space()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "\n",
    "    new_obs, rew, done = env.step(action)\n",
    "    \n",
    "\n",
    "    transition = (obs,action,rew,done,new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    episode_reward += rew\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "\n",
    "    # start gradient step \n",
    "    transitions = random.sample(replay_buffer,BATCH_SIZE)\n",
    "    \n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rewards = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "\n",
    "    obses_t = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    \n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "\n",
    "    # compute targets\n",
    "\n",
    "    target_q_values = target_net(new_obses_t)\n",
    "\n",
    "    max_target_q_values = target_q_values.max(dim=1,keepdim=True)[0]\n",
    "\n",
    "    targets = rewards_t + GAMMA * (1-dones_t) * max_target_q_values\n",
    "\n",
    "    # loss\n",
    "\n",
    "    q_values = online_net(obses_t)\n",
    "\n",
    "    action_q_values = torch.gather(input = q_values, dim=1, index = actions_t)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values,targets)\n",
    "\n",
    "    # gradient step\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update target network if needed\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    # checkpointing\n",
    "    if step % TARGET_SAVE_FREQ == 0:\n",
    "        print(\"Saving target net\")\n",
    "        torch.save(target_net.state_dict(), MODELS_DIR+\"/car_target_net.pth\")\n",
    "    \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        rew_mean = np.mean(reward_buffer)\n",
    "        print()\n",
    "        print('Step', step)\n",
    "        print('Avg Rew',rew_mean)\n",
    "        summary_writer.add_scalar('avg_rew', rew_mean, global_step=step)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
