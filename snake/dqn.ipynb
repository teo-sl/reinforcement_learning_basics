{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.2.0 (SDL 2.0.22, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from game import SnakeGame, ACTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9               \n",
    "BATCH_SIZE = 1000          \n",
    "BUFFER_SIZE = 100_000      \n",
    "MIN_REPLAY_SIZE = 1_000    \n",
    "EPSILON_START = 1.0 \n",
    "EPSILON_END = 0.01\n",
    "EPSILON_DECAY = 5_000      \n",
    "TARGET_UPDATE_FREQ = 1_000  \n",
    "LEARNING_RATE = 0.001\n",
    "TARGET_SAVE_FREQ = TARGET_UPDATE_FREQ*25\n",
    "MODELS_DIR = '../saved_models'\n",
    "LOG_DIR = './logs/snake_1_0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(env.get_observation_space_size(), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, env.get_action_space_size()),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    def act(self,obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self.forward(obs_t.unsqueeze(0))\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = SnakeGame()\n",
    "\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "reward_buffer = deque([],maxlen=100)\n",
    "score_buffer = deque([],maxlen=100)\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniziatlize Replay Buffer\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.sample_from_action_space()\n",
    "\n",
    "    new_obs, rew, done, _ = env.step(action)\n",
    "    \n",
    "    \n",
    "    transition = (obs,action,rew,done,new_obs)\n",
    "    \n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving target net\n",
      "\n",
      "Step 0\n",
      "Avg Rew 0.0\n",
      "\n",
      "Step 1000\n",
      "Avg Rew -8.695652173913043\n",
      "\n",
      "Step 2000\n",
      "Avg Rew -9.387755102040817\n",
      "\n",
      "Step 3000\n",
      "Avg Rew -9.375\n",
      "\n",
      "Step 4000\n",
      "Avg Rew -9.2\n",
      "\n",
      "Step 5000\n",
      "Avg Rew -9.3\n",
      "\n",
      "Step 6000\n",
      "Avg Rew -9.2\n",
      "\n",
      "Step 7000\n",
      "Avg Rew -9.3\n",
      "\n",
      "Step 8000\n",
      "Avg Rew -9.2\n",
      "\n",
      "Step 9000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 10000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 11000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 12000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 13000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 14000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 15000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 16000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 17000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 18000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 19000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 20000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 21000\n",
      "Avg Rew -9.1\n",
      "\n",
      "Step 22000\n",
      "Avg Rew -9.0\n",
      "\n",
      "Step 23000\n",
      "Avg Rew -9.0\n",
      "\n",
      "Step 24000\n",
      "Avg Rew -8.7\n",
      "Saving target net\n",
      "\n",
      "Step 25000\n",
      "Avg Rew -8.4\n",
      "\n",
      "Step 26000\n",
      "Avg Rew -8.4\n",
      "\n",
      "Step 27000\n",
      "Avg Rew -8.3\n",
      "\n",
      "Step 28000\n",
      "Avg Rew -8.3\n",
      "\n",
      "Step 29000\n",
      "Avg Rew -8.5\n",
      "\n",
      "Step 30000\n",
      "Avg Rew -8.4\n",
      "\n",
      "Step 31000\n",
      "Avg Rew -8.4\n",
      "\n",
      "Step 32000\n",
      "Avg Rew -8.3\n",
      "\n",
      "Step 33000\n",
      "Avg Rew -8.3\n",
      "\n",
      "Step 34000\n",
      "Avg Rew -8.2\n",
      "\n",
      "Step 35000\n",
      "Avg Rew -8.2\n",
      "\n",
      "Step 36000\n",
      "Avg Rew -8.1\n",
      "\n",
      "Step 37000\n",
      "Avg Rew -8.1\n",
      "\n",
      "Step 38000\n",
      "Avg Rew -8.1\n",
      "\n",
      "Step 39000\n",
      "Avg Rew -7.7\n",
      "\n",
      "Step 40000\n",
      "Avg Rew -7.6\n",
      "\n",
      "Step 41000\n",
      "Avg Rew -7.4\n",
      "\n",
      "Step 42000\n",
      "Avg Rew -7.5\n",
      "\n",
      "Step 43000\n",
      "Avg Rew -7.6\n",
      "\n",
      "Step 44000\n",
      "Avg Rew -7.5\n",
      "\n",
      "Step 45000\n",
      "Avg Rew -7.5\n",
      "\n",
      "Step 46000\n",
      "Avg Rew -7.5\n",
      "\n",
      "Step 47000\n",
      "Avg Rew -7.5\n",
      "\n",
      "Step 48000\n",
      "Avg Rew -7.4\n",
      "\n",
      "Step 49000\n",
      "Avg Rew -7.3\n",
      "Saving target net\n",
      "\n",
      "Step 50000\n",
      "Avg Rew -7.2\n",
      "\n",
      "Step 51000\n",
      "Avg Rew -7.2\n",
      "\n",
      "Step 52000\n",
      "Avg Rew -7.1\n",
      "\n",
      "Step 53000\n",
      "Avg Rew -7.0\n",
      "\n",
      "Step 54000\n",
      "Avg Rew -6.6\n",
      "\n",
      "Step 55000\n",
      "Avg Rew -6.5\n",
      "\n",
      "Step 56000\n",
      "Avg Rew -6.3\n",
      "\n",
      "Step 57000\n",
      "Avg Rew -5.7\n",
      "\n",
      "Step 58000\n",
      "Avg Rew -5.6\n",
      "\n",
      "Step 59000\n",
      "Avg Rew -4.9\n",
      "\n",
      "Step 60000\n",
      "Avg Rew -3.6\n",
      "\n",
      "Step 61000\n",
      "Avg Rew -3.6\n",
      "\n",
      "Step 62000\n",
      "Avg Rew -3.4\n",
      "\n",
      "Step 63000\n",
      "Avg Rew -3.5\n",
      "\n",
      "Step 64000\n",
      "Avg Rew -0.1\n",
      "\n",
      "Step 65000\n",
      "Avg Rew 0.7\n",
      "\n",
      "Step 66000\n",
      "Avg Rew 3.8\n",
      "\n",
      "Step 67000\n",
      "Avg Rew 5.1\n",
      "\n",
      "Step 68000\n",
      "Avg Rew 9.8\n",
      "\n",
      "Step 69000\n",
      "Avg Rew 12.0\n",
      "\n",
      "Step 70000\n",
      "Avg Rew 16.8\n",
      "\n",
      "Step 71000\n",
      "Avg Rew 18.4\n",
      "\n",
      "Step 72000\n",
      "Avg Rew 24.4\n",
      "\n",
      "Step 73000\n",
      "Avg Rew 31.1\n",
      "\n",
      "Step 74000\n",
      "Avg Rew 37.1\n",
      "Saving target net\n",
      "\n",
      "Step 75000\n",
      "Avg Rew 42.6\n",
      "\n",
      "Step 76000\n",
      "Avg Rew 47.0\n",
      "\n",
      "Step 77000\n",
      "Avg Rew 53.4\n",
      "\n",
      "Step 78000\n",
      "Avg Rew 61.0\n",
      "\n",
      "Step 79000\n",
      "Avg Rew 68.2\n",
      "\n",
      "Step 80000\n",
      "Avg Rew 72.8\n",
      "\n",
      "Step 81000\n",
      "Avg Rew 80.8\n",
      "\n",
      "Step 82000\n",
      "Avg Rew 85.8\n",
      "\n",
      "Step 83000\n",
      "Avg Rew 92.4\n",
      "\n",
      "Step 84000\n",
      "Avg Rew 99.9\n",
      "\n",
      "Step 85000\n",
      "Avg Rew 107.1\n",
      "\n",
      "Step 86000\n",
      "Avg Rew 112.5\n",
      "\n",
      "Step 87000\n",
      "Avg Rew 119.5\n",
      "\n",
      "Step 88000\n",
      "Avg Rew 124.9\n",
      "\n",
      "Step 89000\n",
      "Avg Rew 132.7\n",
      "\n",
      "Step 90000\n",
      "Avg Rew 139.8\n",
      "\n",
      "Step 91000\n",
      "Avg Rew 145.0\n",
      "\n",
      "Step 92000\n",
      "Avg Rew 148.9\n",
      "\n",
      "Step 93000\n",
      "Avg Rew 157.3\n",
      "\n",
      "Step 94000\n",
      "Avg Rew 164.4\n",
      "\n",
      "Step 95000\n",
      "Avg Rew 170.7\n",
      "\n",
      "Step 96000\n",
      "Avg Rew 174.5\n",
      "\n",
      "Step 97000\n",
      "Avg Rew 176.5\n",
      "\n",
      "Step 98000\n",
      "Avg Rew 178.5\n",
      "\n",
      "Step 99000\n",
      "Avg Rew 182.8\n",
      "Saving target net\n",
      "\n",
      "Step 100000\n",
      "Avg Rew 188.9\n",
      "\n",
      "Step 101000\n",
      "Avg Rew 191.3\n",
      "\n",
      "Step 102000\n",
      "Avg Rew 195.0\n",
      "\n",
      "Step 103000\n",
      "Avg Rew 198.7\n",
      "\n",
      "Step 104000\n",
      "Avg Rew 202.6\n",
      "\n",
      "Step 105000\n",
      "Avg Rew 204.3\n",
      "\n",
      "Step 106000\n",
      "Avg Rew 207.5\n",
      "\n",
      "Step 107000\n",
      "Avg Rew 208.9\n",
      "\n",
      "Step 108000\n",
      "Avg Rew 211.5\n",
      "\n",
      "Step 109000\n",
      "Avg Rew 214.5\n",
      "\n",
      "Step 110000\n",
      "Avg Rew 215.0\n",
      "\n",
      "Step 111000\n",
      "Avg Rew 217.1\n",
      "\n",
      "Step 112000\n",
      "Avg Rew 219.0\n",
      "\n",
      "Step 113000\n",
      "Avg Rew 221.5\n",
      "\n",
      "Step 114000\n",
      "Avg Rew 221.6\n",
      "\n",
      "Step 115000\n",
      "Avg Rew 222.0\n",
      "\n",
      "Step 116000\n",
      "Avg Rew 220.9\n",
      "\n",
      "Step 117000\n",
      "Avg Rew 222.2\n",
      "\n",
      "Step 118000\n",
      "Avg Rew 222.7\n",
      "\n",
      "Step 119000\n",
      "Avg Rew 225.2\n",
      "\n",
      "Step 120000\n",
      "Avg Rew 227.2\n",
      "\n",
      "Step 121000\n",
      "Avg Rew 230.7\n",
      "\n",
      "Step 122000\n",
      "Avg Rew 230.5\n",
      "\n",
      "Step 123000\n",
      "Avg Rew 228.9\n",
      "\n",
      "Step 124000\n",
      "Avg Rew 228.6\n",
      "Saving target net\n",
      "\n",
      "Step 125000\n",
      "Avg Rew 230.2\n",
      "\n",
      "Step 126000\n",
      "Avg Rew 233.6\n",
      "\n",
      "Step 127000\n",
      "Avg Rew 233.6\n",
      "\n",
      "Step 128000\n",
      "Avg Rew 237.3\n",
      "\n",
      "Step 129000\n",
      "Avg Rew 235.2\n",
      "\n",
      "Step 130000\n",
      "Avg Rew 236.1\n",
      "\n",
      "Step 131000\n",
      "Avg Rew 241.3\n",
      "\n",
      "Step 132000\n",
      "Avg Rew 241.8\n",
      "\n",
      "Step 133000\n",
      "Avg Rew 239.6\n",
      "\n",
      "Step 134000\n",
      "Avg Rew 240.0\n",
      "\n",
      "Step 135000\n",
      "Avg Rew 236.7\n",
      "\n",
      "Step 136000\n",
      "Avg Rew 234.7\n",
      "\n",
      "Step 137000\n",
      "Avg Rew 232.2\n",
      "\n",
      "Step 138000\n",
      "Avg Rew 232.1\n",
      "\n",
      "Step 139000\n",
      "Avg Rew 233.7\n",
      "\n",
      "Step 140000\n",
      "Avg Rew 233.3\n",
      "\n",
      "Step 141000\n",
      "Avg Rew 234.2\n",
      "\n",
      "Step 142000\n",
      "Avg Rew 234.4\n",
      "\n",
      "Step 143000\n",
      "Avg Rew 236.3\n",
      "\n",
      "Step 144000\n",
      "Avg Rew 237.8\n",
      "\n",
      "Step 145000\n",
      "Avg Rew 238.0\n",
      "\n",
      "Step 146000\n",
      "Avg Rew 237.7\n",
      "\n",
      "Step 147000\n",
      "Avg Rew 237.4\n",
      "\n",
      "Step 148000\n",
      "Avg Rew 235.4\n",
      "\n",
      "Step 149000\n",
      "Avg Rew 234.6\n",
      "Saving target net\n",
      "\n",
      "Step 150000\n",
      "Avg Rew 233.3\n",
      "\n",
      "Step 151000\n",
      "Avg Rew 234.2\n",
      "\n",
      "Step 152000\n",
      "Avg Rew 233.5\n",
      "\n",
      "Step 153000\n",
      "Avg Rew 233.3\n",
      "\n",
      "Step 154000\n",
      "Avg Rew 233.3\n",
      "\n",
      "Step 155000\n",
      "Avg Rew 233.9\n",
      "\n",
      "Step 156000\n",
      "Avg Rew 233.3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     action \u001b[39m=\u001b[39m online_net\u001b[39m.\u001b[39mact(obs)\n\u001b[0;32m---> 14\u001b[0m new_obs, rew, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     16\u001b[0m transition \u001b[39m=\u001b[39m (obs,action,rew,done,new_obs)\n\u001b[1;32m     17\u001b[0m replay_buffer\u001b[39m.\u001b[39mappend(transition)\n",
      "File \u001b[0;32m~/Documents/git_repos/self_driving/snake/game.py:204\u001b[0m, in \u001b[0;36mSnakeGame.step\u001b[0;34m(self, action_dir)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfoody \u001b[39m=\u001b[39m \u001b[39mround\u001b[39m(random\u001b[39m.\u001b[39mrandrange(\u001b[39m0\u001b[39m, DIS_HEIGHT \u001b[39m-\u001b[39m SNAKE_BLOCK) \u001b[39m/\u001b[39m \u001b[39m10.0\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m10.0\u001b[39m\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength_of_snake \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock\u001b[39m.\u001b[39mtick(SNAKE_SPEED)\n\u001b[1;32m    206\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgame_close:\n\u001b[1;32m    207\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mREWARD_UNIT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in itertools.count():\n",
    "    epsilon = np.interp(step,[0,EPSILON_DECAY],[EPSILON_START,EPSILON_END])\n",
    "    rnd_sample = random.random()\n",
    "\n",
    "    if rnd_sample < epsilon:\n",
    "        action = env.sample_from_action_space()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "\n",
    "    new_obs, rew, done, infos = env.step(action)\n",
    "\n",
    "    transition = (obs,action,rew,done,new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    episode_reward += rew\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        score_buffer.append(infos['score'])\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "\n",
    "    # start gradient step \n",
    "    transitions = random.sample(replay_buffer,BATCH_SIZE)\n",
    "    \n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rewards = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "\n",
    "    obses_t = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    \n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "\n",
    "    # compute targets\n",
    "\n",
    "    target_q_values = target_net(new_obses_t)\n",
    "\n",
    "    max_target_q_values = target_q_values.max(dim=1,keepdim=True)[0]\n",
    "\n",
    "    targets = rewards_t + GAMMA * (1-dones_t) * max_target_q_values\n",
    "\n",
    "    # loss\n",
    "\n",
    "    q_values = online_net(obses_t)\n",
    "\n",
    "    action_q_values = torch.gather(input = q_values, dim=1, index = actions_t)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values,targets)\n",
    "\n",
    "    # gradient step\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update target network if needed\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    # checkpointing\n",
    "    if step % TARGET_SAVE_FREQ == 0:\n",
    "        print(\"Saving target net\")\n",
    "        torch.save(target_net.state_dict(), MODELS_DIR+\"/snake_target_net_.pth\")\n",
    "    \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        rew_mean = np.mean(reward_buffer)\n",
    "        score_mean = np.mean(score_buffer)\n",
    "        print()\n",
    "        print('Step', step)\n",
    "        print('Avg Rew',rew_mean)\n",
    "        print('Avg Score',score_mean)\n",
    "        summary_writer.add_scalar('avg_rew', rew_mean, global_step=step)\n",
    "        summary_writer.add_scalar('avg_score', score_mean, global_step=step)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
