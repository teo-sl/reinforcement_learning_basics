{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.2.0 (SDL 2.0.22, Python 3.9.16)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from collections import deque\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from car import CarEnv,WIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99               \n",
    "BATCH_SIZE = 64          \n",
    "BUFFER_SIZE = 100_000      \n",
    "MIN_REPLAY_SIZE = 1_000    \n",
    "EPSILON_START = 1.0 \n",
    "EPSILON_END = 0.1\n",
    "EPSILON_DECAY = 8_000      \n",
    "TARGET_UPDATE_FREQ = 1_000  \n",
    "LEARNING_RATE = 0.001\n",
    "TARGET_SAVE_FREQ = TARGET_UPDATE_FREQ*25\n",
    "MODELS_DIR = '../saved_models'\n",
    "LOG_DIR = '../logs/car_1_1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_writer = SummaryWriter(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(env.get_observation_space_size(), 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, env.get_action_space_size()),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    def act(self,obs):\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32)\n",
    "        q_values = self.forward(obs_t.unsqueeze(0))\n",
    "        max_q_index = torch.argmax(q_values, dim=1)[0]\n",
    "        action = max_q_index.detach().item()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CarEnv()\n",
    "\n",
    "replay_buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "reward_buffer = deque([],maxlen=100)\n",
    "score_buffer = deque([],maxlen=100)\n",
    "episode_reward = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_net = Network(env)\n",
    "target_net = Network(env)\n",
    "target_net.load_state_dict(online_net.state_dict())\n",
    "\n",
    "optimizer = torch.optim.Adam(online_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iniziatlize Replay Buffer\n",
    "obs = env.reset()\n",
    "\n",
    "for _ in range(MIN_REPLAY_SIZE):\n",
    "    action = env.sample_from_action_space()\n",
    "\n",
    "    new_obs, rew, done = env.step(action)    \n",
    "    \n",
    "    transition = (obs,action,rew,done,new_obs)\n",
    "    \n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving target net\n",
      "\n",
      "Step 0\n",
      "Avg Rew nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teodorosullazzo/opt/anaconda3/envs/pytorch_env_2/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3464: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/teodorosullazzo/opt/anaconda3/envs/pytorch_env_2/lib/python3.9/site-packages/numpy/core/_methods.py:192: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1000\n",
      "Avg Rew -630.0\n",
      "\n",
      "Step 2000\n",
      "Avg Rew -405.5\n",
      "\n",
      "Step 3000\n",
      "Avg Rew -516.3333333333334\n",
      "\n",
      "Step 4000\n",
      "Avg Rew -411.0\n",
      "\n",
      "Step 5000\n",
      "Avg Rew -230.42857142857142\n",
      "\n",
      "Step 6000\n",
      "Avg Rew -107.5\n",
      "\n",
      "Step 7000\n",
      "Avg Rew -39.526315789473685\n",
      "\n",
      "Step 8000\n",
      "Avg Rew -5.2592592592592595\n",
      "\n",
      "Step 9000\n",
      "Avg Rew 17.885714285714286\n",
      "\n",
      "Step 10000\n",
      "Avg Rew 31.666666666666668\n",
      "\n",
      "Step 11000\n",
      "Avg Rew 42.5625\n",
      "\n",
      "Step 12000\n",
      "Avg Rew 51.113207547169814\n",
      "\n",
      "Step 13000\n",
      "Avg Rew 62.25\n",
      "\n",
      "Step 14000\n",
      "Avg Rew 72.83333333333333\n",
      "\n",
      "Step 15000\n",
      "Avg Rew 79.6\n",
      "\n",
      "Step 16000\n",
      "Avg Rew 82.65217391304348\n",
      "\n",
      "Step 17000\n",
      "Avg Rew 83.69444444444444\n",
      "\n",
      "Step 18000\n",
      "Avg Rew 89.78666666666666\n",
      "\n",
      "Step 19000\n",
      "Avg Rew 92.96103896103897\n",
      "\n",
      "Step 20000\n",
      "Avg Rew 101.875\n",
      "\n",
      "Step 21000\n",
      "Avg Rew 101.875\n",
      "\n",
      "Step 22000\n",
      "Avg Rew 107.20987654320987\n",
      "\n",
      "Step 23000\n",
      "Avg Rew 111.97560975609755\n",
      "\n",
      "Step 24000\n",
      "Avg Rew 118.75581395348837\n",
      "Saving target net\n",
      "\n",
      "Step 25000\n",
      "Avg Rew 126.94252873563218\n",
      "\n",
      "Step 26000\n",
      "Avg Rew 133.79775280898878\n",
      "\n",
      "Step 27000\n",
      "Avg Rew 139.43333333333334\n",
      "\n",
      "Step 28000\n",
      "Avg Rew 139.43333333333334\n",
      "\n",
      "Step 29000\n",
      "Avg Rew 139.43333333333334\n",
      "\n",
      "Step 30000\n",
      "Avg Rew 162.2173913043478\n",
      "\n",
      "Step 31000\n",
      "Avg Rew 167.95744680851064\n",
      "\n",
      "Step 32000\n",
      "Avg Rew 169.15625\n",
      "\n",
      "Step 33000\n",
      "Avg Rew 172.6060606060606\n",
      "\n",
      "Step 34000\n",
      "Avg Rew 186.08\n",
      "\n",
      "Step 35000\n",
      "Avg Rew 200.25\n",
      "\n",
      "Step 36000\n",
      "Avg Rew 210.21\n",
      "\n",
      "Step 37000\n",
      "Avg Rew 219.16\n",
      "\n",
      "Step 38000\n",
      "Avg Rew 223.52\n",
      "\n",
      "Step 39000\n",
      "Avg Rew 223.94\n",
      "\n",
      "Step 40000\n",
      "Avg Rew 233.31\n",
      "\n",
      "Step 41000\n",
      "Avg Rew 237.61\n",
      "\n",
      "Step 42000\n",
      "Avg Rew 248.02\n",
      "\n",
      "Step 43000\n",
      "Avg Rew 252.49\n",
      "\n",
      "Step 44000\n",
      "Avg Rew 262.07\n",
      "\n",
      "Step 45000\n",
      "Avg Rew 266.49\n",
      "\n",
      "Step 46000\n",
      "Avg Rew 268.16\n",
      "\n",
      "Step 47000\n",
      "Avg Rew 277.97\n",
      "\n",
      "Step 48000\n",
      "Avg Rew 277.22\n",
      "\n",
      "Step 49000\n",
      "Avg Rew 284.58\n",
      "Saving target net\n",
      "\n",
      "Step 50000\n",
      "Avg Rew 295.12\n",
      "\n",
      "Step 51000\n",
      "Avg Rew 299.67\n",
      "\n",
      "Step 52000\n",
      "Avg Rew 305.8\n",
      "\n",
      "Step 53000\n",
      "Avg Rew 308.57\n",
      "\n",
      "Step 54000\n",
      "Avg Rew 307.28\n",
      "\n",
      "Step 55000\n",
      "Avg Rew 314.99\n",
      "\n",
      "Step 56000\n",
      "Avg Rew 314.99\n",
      "\n",
      "Step 57000\n",
      "Avg Rew 334.5\n",
      "\n",
      "Step 58000\n",
      "Avg Rew 344.66\n",
      "\n",
      "Step 59000\n",
      "Avg Rew 351.58\n",
      "\n",
      "Step 60000\n",
      "Avg Rew 355.59\n",
      "\n",
      "Step 61000\n",
      "Avg Rew 364.91\n",
      "\n",
      "Step 62000\n",
      "Avg Rew 370.5\n",
      "\n",
      "Step 63000\n",
      "Avg Rew 375.42\n",
      "\n",
      "Step 64000\n",
      "Avg Rew 374.28\n",
      "\n",
      "Step 65000\n",
      "Avg Rew 377.47\n",
      "\n",
      "Step 66000\n",
      "Avg Rew 378.41\n",
      "\n",
      "Step 67000\n",
      "Avg Rew 382.56\n",
      "\n",
      "Step 68000\n",
      "Avg Rew 393.6\n",
      "\n",
      "Step 69000\n",
      "Avg Rew 392.34\n",
      "\n",
      "Step 70000\n",
      "Avg Rew 390.03\n",
      "\n",
      "Step 71000\n",
      "Avg Rew 392.44\n",
      "\n",
      "Step 72000\n",
      "Avg Rew 383.39\n",
      "\n",
      "Step 73000\n",
      "Avg Rew 359.45\n",
      "\n",
      "Step 74000\n",
      "Avg Rew 341.32\n",
      "Saving target net\n",
      "\n",
      "Step 75000\n",
      "Avg Rew 327.96\n",
      "\n",
      "Step 76000\n",
      "Avg Rew 311.37\n",
      "\n",
      "Step 77000\n",
      "Avg Rew 267.09\n",
      "\n",
      "Step 78000\n",
      "Avg Rew 267.09\n",
      "\n",
      "Step 79000\n",
      "Avg Rew 252.23\n",
      "\n",
      "Step 80000\n",
      "Avg Rew 218.1\n",
      "\n",
      "Step 81000\n",
      "Avg Rew 148.1\n",
      "\n",
      "Step 82000\n",
      "Avg Rew 120.57\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     action \u001b[39m=\u001b[39m online_net\u001b[39m.\u001b[39mact(obs)\n\u001b[0;32m---> 14\u001b[0m new_obs, rew, done \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     16\u001b[0m transition \u001b[39m=\u001b[39m (obs,action,rew,done,new_obs)\n\u001b[1;32m     17\u001b[0m replay_buffer\u001b[39m.\u001b[39mappend(transition)\n",
      "File \u001b[0;32m~/Documents/git_repos/self_driving/car/car.py:328\u001b[0m, in \u001b[0;36mCarEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    326\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    327\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclock\u001b[39m.\u001b[39mtick(FPS)\n\u001b[0;32m--> 328\u001b[0m draw(WIN, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimages, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplayer_car)\n\u001b[1;32m    330\u001b[0m old_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_car\u001b[39m.\u001b[39mx\n\u001b[1;32m    331\u001b[0m old_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplayer_car\u001b[39m.\u001b[39my\n",
      "File \u001b[0;32m~/Documents/git_repos/self_driving/car/car.py:110\u001b[0m, in \u001b[0;36mdraw\u001b[0;34m(win, images, player_car)\u001b[0m\n\u001b[1;32m    107\u001b[0m     win\u001b[39m.\u001b[39mblit(img, pos)\n\u001b[1;32m    109\u001b[0m player_car\u001b[39m.\u001b[39mdraw(win)\n\u001b[0;32m--> 110\u001b[0m pygame\u001b[39m.\u001b[39;49mdisplay\u001b[39m.\u001b[39;49mupdate()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "obs = env.reset()\n",
    "\n",
    "for step in itertools.count():\n",
    "    epsilon = np.interp(step,[0,EPSILON_DECAY],[EPSILON_START,EPSILON_END])\n",
    "    rnd_sample = random.random()\n",
    "\n",
    "    if rnd_sample < epsilon:\n",
    "        action = env.sample_from_action_space()\n",
    "    else:\n",
    "        action = online_net.act(obs)\n",
    "\n",
    "    new_obs, rew, done = env.step(action)\n",
    "\n",
    "    transition = (obs,action,rew,done,new_obs)\n",
    "    replay_buffer.append(transition)\n",
    "    obs = new_obs\n",
    "\n",
    "    episode_reward += rew\n",
    "\n",
    "    if done:\n",
    "        obs = env.reset()\n",
    "        reward_buffer.append(episode_reward)\n",
    "        episode_reward = 0.0\n",
    "\n",
    "\n",
    "    # start gradient step \n",
    "    transitions = random.sample(replay_buffer,BATCH_SIZE)\n",
    "    \n",
    "    obses = np.asarray([t[0] for t in transitions])\n",
    "    actions = np.asarray([t[1] for t in transitions])\n",
    "    rewards = np.asarray([t[2] for t in transitions])\n",
    "    dones = np.asarray([t[3] for t in transitions])\n",
    "    new_obses = np.asarray([t[4] for t in transitions])\n",
    "\n",
    "\n",
    "    obses_t = torch.as_tensor(obses, dtype=torch.float32)\n",
    "    \n",
    "    actions_t = torch.as_tensor(actions, dtype=torch.int64).unsqueeze(-1)\n",
    "    rewards_t = torch.as_tensor(rewards, dtype=torch.float32).unsqueeze(-1)\n",
    "    dones_t = torch.as_tensor(dones, dtype=torch.float32).unsqueeze(-1)\n",
    "    new_obses_t = torch.as_tensor(new_obses, dtype=torch.float32)\n",
    "\n",
    "    # compute targets\n",
    "\n",
    "    target_q_values = target_net(new_obses_t)\n",
    "\n",
    "    max_target_q_values = target_q_values.max(dim=1,keepdim=True)[0]\n",
    "\n",
    "    targets = rewards_t + GAMMA * (1-dones_t) * max_target_q_values\n",
    "\n",
    "    # loss\n",
    "\n",
    "    q_values = online_net(obses_t)\n",
    "\n",
    "    action_q_values = torch.gather(input = q_values, dim=1, index = actions_t)\n",
    "\n",
    "    loss = nn.functional.smooth_l1_loss(action_q_values,targets)\n",
    "\n",
    "    # gradient step\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update target network if needed\n",
    "    if step % TARGET_UPDATE_FREQ == 0:\n",
    "        target_net.load_state_dict(online_net.state_dict())\n",
    "        \n",
    "    # checkpointing\n",
    "    if step % TARGET_SAVE_FREQ == 0:\n",
    "        print(\"Saving target net\")\n",
    "        torch.save(target_net.state_dict(), MODELS_DIR+\"/snake_target_net_.pth\")\n",
    "    \n",
    "    # Logging\n",
    "    if step % 1000 == 0:\n",
    "        rew_mean = np.mean(reward_buffer)\n",
    "        print()\n",
    "        print('Step', step)\n",
    "        print('Avg Rew',rew_mean)\n",
    "        summary_writer.add_scalar('avg_rew', rew_mean, global_step=step)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
